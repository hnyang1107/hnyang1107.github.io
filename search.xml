<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[剑指 offer 题目总结：树]]></title>
    <url>%2Fposts%2F10808%2F</url>
    <content type="text"><![CDATA[文章目录######]]></content>
  </entry>
  <entry>
    <title><![CDATA[剑指 offer 题目总结：线性表]]></title>
    <url>%2Fposts%2F38192%2F</url>
    <content type="text"><![CDATA[文章目录1. 数组面试题 3：数组中重复数字2. 链表1. 数组面试题 3：数组中重复数字在一个长度为 $n$ 的数组里的所有数字都在 $0$~$n-1$ 的范围内。数组中某些数字是重复的,但不知道有几个数字重复了,也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。例如,如果输入长度为7的数组 {2,3,1,0,2,5,3} 那么对应的输出是重复的数字 2 或者 3 。2. 链表]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>线性表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构知识点拾遗]]></title>
    <url>%2Fposts%2F53392%2F</url>
    <content type="text"><![CDATA[文章目录1.数据结构部分1.1 Hash表（哈希表）1.2 深度优先搜索（DFS）和广度优先搜索（BFS）2.算法部分2.1 算法的时间复杂度1.数据结构部分1.1 Hash表（哈希表）1.2 深度优先搜索（DFS）和广度优先搜索（BFS）2.算法部分2.1 算法的时间复杂度算法的时间复杂度（Time complexity）是一个关于输入变量规模 $n$ 函数，它定性描述该算法的运行时间，只考虑这个函数的量级用来表示算法所花费的相对时间，分为 最好时间复杂度，最坏时间复杂度，平均时间复杂度，一般只关心最坏和平均的情况，对于复杂的算法，求平均时间复杂度会牵涉到复杂的概率计算，一般时间复杂度指的是最坏时间复杂度.1.不能只看 for 循环的个数考虑如下代码：算法 2.1：判断三个数组是否有交集12345678def disjoint1(A, B, C): ”””Return True if there is no element common to all three lists.””” for a in A: for b in B: for c in C: if a == b == c: return False # we found a common value return True # if we reach this, sets are disjoin很自然，(2.1) 的时间复杂度为 $O(n^3)$，实际上，如果 $A$ 或 $B$ 有相同的数 $x$，那么直接拿 $x$ 在 $C$ 中去遍历查找即可：算法 2.2：判断三个数组是否有交集123456789def disjoint2(A, B, C): ”””Return True if there is no element common to all three lists.””” for a in A: for b in B: if a == b: # only check C if we found match from A and B for c in C: if a == c: # (and thus a == b == c) return False # we found a common value return True # if we reach this, sets are disjoin这里的时间复杂度为 $O(n^2+n)=O(n^2)$]]></content>
      <tags>
        <tag>LeetCode</tag>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机(Support Vector Machine)：原理与推导]]></title>
    <url>%2Fposts%2F33807%2F</url>
    <content type="text"><![CDATA[“A powerful discriminative classifier defined by a separating hyperplane”文章目录1. SVM 基本型1.1 超平面与线性可分1.2 函数间隔与几何间隔1.3 利用间隔最大化推导基本型2. Hard Margin SVM 求解2.1 支持向量与间隔边界2.2 Lagrange Duality2.3 KKT 条件与支持向量3. Soft Margin SVM 求解4. 线性不可分时的 Kernel 方法多分类问题参考资料1. SVM 基本型1.1 超平面与线性可分考虑的是一个两类的分类问题，数据点用 $\boldsymbol{x}$ 来表示，这是一个 $n$ 维向量，而类别用 $y$ 来表示，可以取 $0$ 或者 $-1$ ，分别代表两个不同的类（不同于 Logistics Regression 的 $0,1$ 有概率意义，这里只是为了方便 SVM 的推导）. 一个线性分类器就是要在 $n$ 维的数据空间中找到一个 $n-1$ 维的超平面，其方程可以表示为$$w^{T}\boldsymbol{x}+b=0\tag{1.1}$$一般我们的向量没有加以说明都是指的列向量，所以这里乘法指的是矩阵乘法我们希望这个超平面能够把这两类数据完美的分开来，一边函数值大于 $0$ 为一类一边小于 $0$ 为一类，不存在等于 $0$ 的那一类，如下图所示图中为二维空间的点，我们的目标是找到一个一维空间的超平面（直线）将两类样本分开，这个超平面有两个基本性质：超平面乘上不为 $0$ 的系数 $\lambda$ 也是同一个超平面 $\lambda (w^{T}\boldsymbol{x}+b)=0$$w$ 是超平面的法向量，这个法向量的方向会在平面的任意一侧第二点对后面推导几何间距有用，这里给出一个简单的证明：对任意超平面上的两点 $\boldsymbol{x_1},\boldsymbol{x_2}$ 都会有 $w^{T}(\boldsymbol{x_1}-\boldsymbol{x_2})=0$ 成立，也就是说对于任意属于超平面的向量 $(\boldsymbol{x_1}-\boldsymbol{x_2})$ 都与 $w$ 正交，所以 $w$ 是超平面的法向量. 对于第一点的证明是显然的，单纯看超平面我们没有办法去确定 $w,b$ 似乎需要做一些归一化操作，所以我们的目标是定义和找到合适的 $w,b$ 来达到我们分离两类样本的目的.1.2 函数间隔与几何间隔从几何直观上来说，由于超平面是用于分隔两类数据的，越接近超平面的点越“难”分隔，因为如果超平面稍微转动一下，它们就有可能跑到另一边去。反之，如果是距离超平面很远的点，例如图中的右上角或者左下角的点，则很容易分辩出其类别，所以点离离超平面越远则确信度越高，这里有两种方式来刻画离超平面的远近，数值上 $|w^{T}\boldsymbol{x}+b|$ 可以刻画，在超平面上为 $0$，这就是样本点的函数间隔(functional margin)的概念，对于样本 $i$ ，函数间隔可以方便的写成$$\hat{\gamma}_i = y_i(w^{T}\boldsymbol{x}+b)\tag{1.2}$$也就是说把函数值 $f(\boldsymbol{x}) = (w^{T}\boldsymbol{x}+b)$ 小于 $0$ 的定义为 $-1$ 类，大于 $0$ 的定义为 $1$ 类，对于所有样本都可以求得其函数间隔，对于整体我们只关心最小的那个，因为越大的事分类的越好，控制最小的那个来控制靠近超平面的样本点，所以把训练集的函数间隔定义为$$\hat{\gamma}=\min_{i=1,\cdots,m}\hat{\gamma}_{i}\tag{1.3}$$但是，在 1.1 节中说过，超平面不加限定其系数可以任意取，所以函数间隔可以取任意大小，所以我们需要给 $w$ 加上某些约束($W$ 定下来 $b$ 就确定了) 如进行归一化 $\|w\|=1$，实际上这样归一化后的结果就是几何间隔(geometric margin)直观上，不管超平面的形式如何，点到超平面的距离是确定的，还下图所示，现推导点 $\boldsymbol{x}$ 到超平面的距离 $\gamma$：几何距离的推导：假设 $\boldsymbol{x}$ 在超平面的投影为 $\boldsymbol{x}_{0}$ 由于 $w$ 是法向量，故有 $$\boldsymbol{x}=\boldsymbol{x}_{0}+\gamma\frac{w}{\|w\|}\tag{1.4}$$等式两边同时取函数 $f(\boldsymbol{x}) = (w^{T}\boldsymbol{x}+b)$，注意到 $f(\boldsymbol{x}_0)=0,\|w\|^2=w^Tw$，可得$$\gamma=\frac{|w^{T} \boldsymbol{x}+b|}{\|w\|}=\frac{yf(\boldsymbol{x})}{\|w\|}=\frac{\hat{\gamma}}{\|w\|}\tag{1.5}$$这也是就是说几何间距就是函数间距归一化的结果，这里要加上绝对值，也就是乘上其类别标记 $y$，因为 $\boldsymbol{x_0}$ 与 $w$ 不一定在同一侧，这种几何间距与超平面的比例系数无关（可自行验证）. 同样我们把样本点的最小几何间距定义为训练集的几何间距：$${\gamma}=\min_{i=1,\cdots,m}{\gamma}_{i}\tag{1.6}$$1.3 利用间隔最大化推导基本型直觉上，我们希望所有的点离超平面都足够的远，一方面是增加分类的置信度，另一方面也是增加数据的扰动性（超平面改变一点角度离超平面很近的点分类可能完全改变），也就是说我们优化的目标就是最大化训练样本的间隔，由于函数间隔的不确定性，所以我们选择最大化训练样本的几何间隔，即$$\begin{aligned}&amp;\max_{w, b}\min_{\boldsymbol{x}_{i}}\frac{y_i(w^{T}\boldsymbol{x}_{i}+b)}{\|w\|}\\&amp;\text{ s.t. }\quad y_{i}\left(w^T \cdot \boldsymbol{x}_{i}+b\right)&gt;0, \quad i=1,2, \cdots, m\end{aligned}\tag{1.7}$$上式约束条件严格大于 $0$，表示完全分类准确，此时这种间隔称为硬间隔(soft margin)，软间隔与硬间隔问题在后面会再次详细说明.观察上式的目标函数，$\|w\|$可以放在 $\min$ 外，而约束条件是一个函数间隔，刚刚说过可以取任意值，为了方便，我们不妨设$\min_{\boldsymbol{x}_{i}}y_i(w^{T}\boldsymbol{x}_{i}+b)=r&gt;0$，我们可以令 $r=1$，也就是训练集的函数距离固定成 $\hat{\gamma}=1$, 则优化问题可以写成$$\begin{aligned}&amp;\max_{w, b}\frac{1}{\|w\|}\\&amp;\text{ s.t. }\quad y_{i}\left(w^T \cdot \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \cdots, m\end{aligned}\tag{1.8}$$把目标函数改一下，变成易于求导的形式：$$\begin{aligned}&amp;\min_{w, b}\frac{1}{2}{\|w\|^{2}}=\frac{1}{2}w^Tw\\&amp;\text{ s.t. }\quad y_{i}\left(w^T \cdot \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \cdots, m\end{aligned}\tag{1.9}$$上面的 $\frac{1}{2}$ 只是为了方便求导后消去系数，这样我们通过固定函数间距去最小化几何间距，得到 SVM 的基本型2. Hard Margin SVM 求解2.1 支持向量与间隔边界可以明显地看出来，它是一个凸优化问题，或者更具体地说，它是一个二次优化问题——目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的 QP (Quadratic Programming) 的优化包进行求解. 但是维数很高的话有一种更高效的方式来处理，这就是通过 (Lagrange Duality) 变换到对偶变量 (dual variable) 的优化问题之后，可以找到一种更加有效的方法来进行求解——这也是在神经网络之前 SVM 盛行的一大原因，通常情况下这种方法比直接使用通用的 QP 优化包进行优化要高效得多，在求解之前，我们先来看为什么这个模型叫做支持向量机. 首先，根据刚刚的规定，$\hat{\gamma}=1$，也就是说边界上的线要满足 $y_i(w^{T}\boldsymbol{x}_{i}+b)=1$，所以边界一定是 $w^{T}\boldsymbol{x}_{i}+b=\pm 1$我们要找的超平面与边界平行，而且一定处于边界中间的那个平面，这是因为最优超平面一旦靠近某一边界整个训练集的几何间距就会变小，但实际上我们是要最大化这个几何间距，所以边界决定了超平面的位置，边界又是由一些少量的点决定，边界外的点对整个模型求解没有影响，我们把边界上的点叫做支持向量，$H_1$ 与 $H_2$ 之间的距离成为间隔，也就是最大化这个 Gap 之间的距离.这个优化问题有一个基本结论：最大间隔分离超平面存在唯一性[1]若数据集 $T$ 线性可分，则可将训练数据集中的样本点完全正确分开且分开的最大超平面存在唯一2.2 Lagrange Duality我们把基本型叫做原问题(primal problem)，每一个约束条件加上一个 Lagrange multiplier，目标是把有约束的优化变成对于 $w,b$ 无约束的优化：$$\mathcal{L}(w, b, \alpha)=\frac{1}{2}w^{T}w+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(w^{T} \boldsymbol{x}_{i}+b\right)\right)\tag{2.1}$$然后$$\begin{aligned}&amp;\min_{w, b}\max_{\alpha_{i}}\mathcal{L}(w, b, \alpha)\\&amp;\text{ s.t. }\alpha_{i}\geqslant 0\end{aligned}\tag{2.2}$$这种方法就是将将约束融合到目标函数里，因为现在 $w,b$ 无约束，我们可以把 $w,b$ 的取值分为两部分$P_{1},P_{2}$，$P_{1}$ 满足 $1-w^{T}\boldsymbol{x}_{i}+b&gt;0$ 和 $P_2$ 满足 $1-w^{T}\boldsymbol{x}_{i}+b\leqslant0$，因为我们要在调整 $\alpha_{i}$ 来最大化 $\mathcal{L}$，在 $P_1$ 处显然最大值为 $\infty$，最后我们要去最小化一个含有无穷的项，显然在 $P_1$ 部分是找不到的，所以自然而然的最优解落在了 $P_2$，也就对应了原问题的约束，通过这种方法就可以把有约束的优化变成无约束的优化.然后这里交换最大最小的顺序，根据对偶性，原问题的对偶问题是极大极小问题，一般来说 $\min\max\leqslant\max\min$ 的，直观上也很好理解，最大值中最小的一个总也比最小值中最大的一个要大，但是有的时候是相等的，具体来说，就是要满足 Slater 条件，详细可以参考博文：机器学习中的数学基础，我们这里的问题是满足 Slater 定理的，因此现在我们便转化为求解第二个问题：$$\begin{aligned}&amp;\max_{\alpha_{i}}\min_{w, b}\mathcal{L}(w, b, \alpha)\\&amp;\text{ s.t. }\alpha_{i}\geqslant 0\end{aligned}\tag{2.3}$$ 这样的话先考虑 $\min$ 问题就变成彻底无约束优化，现在需要求 $\mathcal{L}(w, b, \alpha)$ 对 $w,b$ 的最小，由于是凸的，所以直接求极小值：$$\nabla_W \mathcal{L}(w,b,\alpha) = w - \sum_{i=1}^m \alpha_i y_i \boldsymbol{x}_i = 0 \implies w= \sum_{i=1}^m \alpha_i y_i \boldsymbol{x}_i\tag{2.4}$$ 注意到这里的 $w$ 是向量，要对其求向量的梯度，然后对 $b$ 求偏导$$\nabla_b \mathcal{L}(w,b,\alpha) = - \sum_{i=1}^m \alpha_i y_i = 0 \implies \sum_{i=1}^m \alpha_i y_i = 0 \tag{2.5}$$ (2.5) 式子不含 $w,b$，所以变成了我们极大问题的约束条件，把 (2.4) 式带回 (2.1) 加上负号转化为求最小值问题，$$\begin{aligned}\max_{\alpha}&amp;\frac 1 2 \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \boldsymbol{x}_i^T \boldsymbol{x}_j \ - \ \sum_{i=1}^m \alpha_i\\\text{ s.t. }&amp;\alpha_{i}\geqslant 0,\sum_{i=1}^m \alpha_i y_i = 0\end{aligned}\tag{2.6}$$ 其中具体细节的推导可以参考资料[4]，(2.6) 式是一个约束条件相对比较简单的二次规划问题TODO:补充解的形式2.3 KKT 条件与支持向量我们再来看拉格朗日函数的 KKT 条件，(2.1) 的 KKT 条件为：3. Soft Margin SVM 求解4. 线性不可分时的 Kernel 方法多分类问题参考资料[1] 统计学习方法, 李航, 清华大学出版社, 第二版, 2019[2] 支持向量机: Maximum Margin Classifier, 张驰原, pluskid.org, 2010, [Link][3] 机器学习, 周志华，清华大学出版社, 第一版, 2016[Link][4] 机器学习-白板推导系列(六)-支持向量机SVM(Support Vector Machine),shuhuai008, bilibili.com, 2018 [Link][PDF]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>CS229</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库与 MySQL 笔记：基础]]></title>
    <url>%2Fposts%2F9977%2F</url>
    <content type="text"><![CDATA[“Get the dolphin up to speed”文章目录1. 数据库相关概念2. 连接数据库3. SQL 基本命令4. MySQL 语法规范5. DQL (Data Query Language)5.1 基础查询5.2 条件查询5.3 排序查询5.4 分组查询5.5 SQL92 与 99标准的连接查询（多表查询）5.5.1 SQL92 标准：等值连接 [E71-72]5.5.2 SQL92 标准：非等值连接 [E73]5.5.3 SQL92 标准：自连接 [E74]5.5.4 SQL99标准5.5.4.1 SQL99标准：内连接——等值连接5.5.4.1 SQL99标准：内连接——非等值连接[E80]5.5.4.1 SQL99标准：内连接——自连接[E80]5.5.4.1 SQL99标准：左外右外[E82]5.5.4.2 SQL99标准：全外[E83]5.5.4.2 SQL99标准：交叉连接[E84]5.5.5 SQL92与SQL99对比总结5.6 子查询（内查询）5.6.1 WHERE 或 HAVING 后面的子查询5.6.2 SELECT 后面的子查询5.6.3 FROM 后面的子查询5.6.4 EXISTS 后面的子查询 [E93]5.7 分页查询5.8 联合查询6. DML (Data Manipulation Language)6.1 插入语句6.2 修改语句6.2.1 修改单表的记录6.2.2 修改单表的记录6.3 删除语句[E108]7. DDL (Data Define Language)7.1 库的管理7.2 表的管理8. TCL (Transaction Control Language)9. SQL 常用函数9.1 单行函数9.1.1 字符函数9.1.2 数学函数9.1.3 日期函数9.1.4 其他函数9.1.5 流程控制函数9.2 分组函数（统计函数）参考资料1. 数据库相关概念DB: 数据库（database） 存储数据的“仓库”，保存了一系列有组织的数据DBMS: 数据库管理系统（Database Management System）用于管理DB中的数据，数据库是通过DBMS创建和操作的容器，可分为两类：1.基于共享文件系统（Access）2.基于客户机 [mysql] -服务端 [mysqld]（MySQL、Oracle ）(数据安装在服务端)SQL: 结构化查询语言（Structure Query Language）专门用来与数据库通信的语言不是某个特定数据库供应商专有的语言，几乎所有 DBMS 都支持 SQL2. 连接数据库mysql -h (host) + -P (port) + -u (user) + -p (passport)3. SQL 基本命令查看当前所有数据库：SHOW DATABASE;打开指定的库：USE 库名;查看当前库的所有表：SHOW TABLES;查看其他库的所有表：SHOW TABLES FROM 库名;创建表：12345CREATE TABLE 表名( 列名 列类型, 列名 列类型, ...);查看表结构：desc 表名;查询当前操作的数据库：SELECT DATABASE();查看服务器版本：SELECT VERSION();4. MySQL 语法规范不区分大小写，建议关键字大写，表名列名小写每条命令用分号结尾缩进换行不影响代码运行一个空格两个空格是有区别的相对宽容，不是没报错说明语法正确，要看具体的值注释：单行注释：#注释内容 或 -- 注释内容多行注释：/* 注释内容 */5. DQL (Data Query Language)5.1 基础查询1234SELECT 查询列表 FROM 表名;查询表中的单个字段：SELECT last_name FROM employees查询表中的多个字段： SELECT last_name,salary FROM employees字段顺序不需要和表中一致查询表中所有字段：SELECT * FROM employees查询常量值：SELECT 100、SELECT &#39;job&#39;SQL 中不区分字符串和字符，统称为字符型查询表达式：SELECT 100%98查询函数：SELECT VERSION();起别名：SELECT last_name AS 姓;（AS 紧跟着字段，可以省略成空格）注意：可以对字段起别名，也可以对表名起别名别名相当于把原来字段做了替换，原来名称不再有效去重：SELECT DISTINCT department_id FROM employees+ 号作用：SELECT 100+90 两个操作数都为数值型，加法运算SELECT &#39;100&#39;+90 [190] 只要其中一个为字符型，试图将字符型转换成数值型，如果转换成功，继续做加法运算SELECT &#39;100&#39;+90 [90] 如果转换失败，将字符型数值转换成 0SELECT null+10 [null] 其中一方为 null，结果为 null5.2 条件查询123456SELECT 查询列表 -- 3FROM 表名 -- 1WHERE 筛选条件; -- 2按条件表达式筛选条件运算符：&gt; &lt; = != (&lt;&gt;) &gt;= &lt;=按逻辑运算符逻辑运算符：AND OR NOT模糊查询：LIKE: 包含某些字符，一般和统配符搭配：% 任意多个字符，包含 0 个字符_ 任意单个字符在 5.5 版本以上，LIKE 也可以用于数值型数据WHERE 不光可以筛选某字段满足某一个值，也可以满足某一列值BETWEEN 100 AND 200：找到 100 到 200 之间的值，包含边界值，边界值不能颠倒IN (&#39;a&#39;,&#39;b&#39;,&#39;c&#39;): 判断某字段是否属于 in 列表中的某一项，in 列表的值类型必须一致或兼容，不能包含通配符兼容： 类型可以互相转化，例如 123 与 ‘123’IS NULL,IS NOT NULL : 判断是否为 NULL等于 = 、安全等于 &lt;=&gt;、与 IS:等于 = 用来判断普通的数值，不能判断 NULLIS 只能用来判断 NULL安全等于 &lt;=&gt; 既可以 判断 NULL，又可以判断普通的数值，但可读性较低5.3 排序查询12345678SELECT 查询列表 FROM 表名 [WHERE] 筛选条件 ORDER BY 排序列表1 [ASC|DESC], 排序列表2 [ASC|DESC]; (不写默认 ASC)ASC 表示升序，DESC 表示降序ORDER BY 子句中支持单个字段、多个字段、表达式、函数、别名ORDER BY 子句一般是放在查询语句的最后面， LIMIT 子句除外5.4 分组查询123456789101112SELECT -- 5 查询列表或表达式, group_function(column)FROM -- 1 表名 [WHERE] -- 2 前的筛选条件 [GROUP BY] -- 3 分组条件1, 分组条件2（可以是字段，也可以是某字段的单行函数，没有顺序）[HAVING] -- 4 分组后的筛选[ORDER BY] -- 6 排序列表1 [ASC|DESC], 排序列表2 [ASC|DESC]; (不写默认 ASC)分组之前的列筛选，可以用 WHEREWHERE 关键字无法与聚合函数一起使用，对 GROUP BY 之后的新表进行筛选，应该用在 GROUP BY 之后的 HAVINGHAVING 只是对这种情况使用，若果有其它情况要多次筛选，用 WHERE AND对新出现的字段进行筛选 (如MAX(salary)) 要用 HAVING分组函数作条件肯定放在 HAVING 中能写 WHERE 尽量写 WHERE，而不是选择放在后面的 HAVING，这样性能更优先考虑查什么，再写基本查询框架，再考虑筛选条件放哪里5.5 SQL92 与 99标准的连接查询（多表查询）同时查询来源于多个表的多个字段，没有限定条件结果则会成为笛卡尔积的形式 (SQL92标准，99标准用 CROSS JOIN)，加上WHERE条件之后，在笛卡尔乘积表里面进行筛选，FROM 多个表没有顺序关系，只是告诉系统字段来自于这些表，如果有相同字段则要加上前缀区分5.5.1 SQL92 标准：等值连接 [E71-72]多表等值连接的结果为多表的交集部分$n$ 表连接至少需要 $n-1$ 个连接条件多表的顺序没有要求可以给表取别名减少代码冗余可以配合其他子句使用5.5.2 SQL92 标准：非等值连接 [E73]此时 WHERE 后面不是等于，一般是判断属于两个值之间5.5.3 SQL92 标准：自连接 [E74]一张表的某两列有逻辑关系，可以看做自己和自己的等值连接这个时候用别名避免歧义5.5.4 SQL99标准12345678SELECT 查询列表FROM 表1 别名 连接类型 JOIN 表2 别名ON 连接条件[WHERE 筛选条件][GROUP BY][HAVING][ORDER BY]这样写和 92 标准比更加清除，防止 WHERE 结构夹杂在一起连接类型：内连接 inner外连接左外：left [outer]右外：right [outer]全外：full [outer]交叉连接 cross5.5.4.1 SQL99标准：内连接——等值连接内连分为：等值、非等值、自连接也可以添加排序、分组、筛选INNER 可以省略筛选条件放在 where 后面，连接条件放在 on 后面，提高分离性，便于阅读inner join 连接和 SQL92 语法中等值连接效果是一样的，都是查询多表的交集5.5.4.1 SQL99标准：内连接——非等值连接[E80]与 5.5.2 一样，只不过用 JOIN ON5.5.4.1 SQL99标准：内连接——自连接[E80]与 5.5.3一样，只不过用 JOIN ON5.5.4.1 SQL99标准：左外右外[E82]应用场景： 用于查询一个表中有，另一个表没有的记录特点：外连接的查询结果为主表中所有记录，如果从表中有和主表匹配的，则显示匹配的值，若没有，则显示 NULL：外连接查询结果 = 内连接结果 + 主表中有而从表中没有的记录左外连接： left out join 左边的是主表右外连接： right out join 右边是主表主表相当于主要的表，结果与主表行数一致，从表和主表匹配，主表里没有的就用 NULL，5.5.4.2 SQL99标准：全外[E83]相当于左外右外 + 内连接 的合并结果，不匹配的全部用 NULL 填充注意：全外 MySQL 不支持5.5.4.2 SQL99标准：交叉连接[E84]CROSS JOIN 相当于笛卡尔乘积5.5.5 SQL92与SQL99对比总结SQL99 支持功能跟多，可读性更高，建议用 99 语法，下图是这些连接的示意图：5.6 子查询（内查询）含义： 出现在其他语句中的select 语句，称为子查询或内查询，外部的查询语句，称为主查询或外查询分类：按子查询出现的位置：select 后面：仅仅支持标量子查询from 后面：仅仅支持表子查询where 或 having 后面：标量子查询或列子查询或 行子查询exists 后面（相关子查询）：表子查询按结果集出现的行列数不同：标量子查询：结果只有一行一列列子查询：结果只有一列多行行子查询：结果集只有一行多列表子查询：结果集一般为多行多列5.6.1 WHERE 或 HAVING 后面的子查询后面常常搭配着标量子查询（也叫单行子查询）列子查询（也叫多行子查询）(多需要配合`DISTINCT`提高效率)行子查询（也叫多列多行子查询）多行比较符：in/not in : 等于表中的任意一个any|some: 和子查询返回的某一个值比较all: 和子查询返回的所有值比较特点：子查询都放在小括号内、子查询放在条件右侧标量子查询搭配单行操作符（&gt;, &lt;, &gt;=, &lt;=, =, &lt;&gt;）、列子查询搭配多行操作符（in/not in,any/some,all）、子查询的执行优先于主查询执行，主查询的条件用到了子查询的结果列子查询通常配合去重，提高效率5.6.2 SELECT 后面的子查询一般可以用其他方式代替，可用于创建新字段，如：[E99] 第五题5.6.3 FROM 后面的子查询将子查询结果充当一张表，要求必须起别名一般配合多表查询使用，连接这张新表可以放在 JOIN 里，也可以放在 FROM 里5.6.4 EXISTS 后面的子查询 [E93]语法： EXISTS(完整的查询语句)结果：1 表非空或 0表为空一般能用 EXISTS 都能用 in 代替，用的比较少5.7 分页查询应用场景：要显示的数据，一页显示不全，需要分页提交 SQL 请求语法：123456789SELECT 查询列表或表达式, group_function(column)FROM 表名 [WHERE] 前的筛选条件 [GROUP BY] 分组条件1, 分组条件2（可以是字段，也可以是某字段的单行函数，没有顺序）[HAVING] 分组后的筛选[ORDER BY] 排序列表1 [ASC|DESC], 排序列表2 [ASC|DESC]; (不写默认 ASC)[LIMIT] [offset]（要显示的起始索引）,size（要显示的条目个数）;+ `LIMIT` 不论在语法上还是在执行上都是最后+ 公式：要显示的页数 page（第 page 页），每页条目数 size，语句应该是 `LIMIT (page-1)*size,size`注意：这里的起始索引从 0 开始，若从 0 开始，可以省略5.8 联合查询语法：12345查询语句1UNION查询语句2UNION...应用场景：要查询的结果来自于多张表，且多个表没有直接的连接关系，但查询的信息一致特点：字段名默认为第一条查询语句的字段名多条查询语句的查询列数是一致的多条查询语句的查询的每一列的类型和顺序最好要一致，虽然不会报错UNION 关键字会去重，若不要去重，用 UNION ALL6. DML (Data Manipulation Language)6.1 插入语句方式一：12INSERT INTO 表名[(列名, ...)] # 省略表示所有列 VALUES(值1,...)插入的值的类型要与列的类型一致或者兼容不可以为 NULL 得列必须插入值，可以为 NULL 的列列数和插入的值个数要一致省略列名，默认所有列，顺序与原表一致VALUES 后面接子查询的话可以省略可以支持多行，在 后面添加 (值1,值2,...),(值1,值2,...)方式二：12INSERT INTO 表名SET 列名=值,列名=值,...方式一插入支持子查询6.2 修改语句6.2.1 修改单表的记录语法：123UPDATE 表名SET 列名=值,列名=值,...WHERE 筛选条件;6.2.2 修改单表的记录语法：1234UPDATE 表1JOIN 表2 ON 连接规则 # 99 语法，可以相应地改成 92 语法SET 列名=值,列名=值,...WHERE 筛选条件;6.3 删除语句[E108]方式一：单表删除：DELETE FROM 表名 WHERE 筛选条件多表删除：表连接：1234DELETE 表1的别名，表2的别名FROM 表1 别名INNER JOIN 表2别名 ON 连接规则 # 99 语法，可以相应地改成 92 语法WHERE 筛选条件;注意： 单表删除 DELETE 后面不能加字段，多表删除后面得加需要删除的表名方式二：：TRUNCATE 整个表删除，不能加筛选条件DELETE 和 TRUNCATE 的区别：TRUNCATE 效率高一点加入用 DELETE 全部删除整张表后，再插入数据，自增长列的值从断点开始；若用 TRUNCATE 删除后，再插入数据，自增长的值从 1 开始TRUNCATE 删除没返回值，DELETE 有返回值，返回受影响的行数TRUNCATE 删除不能回滚，DELETE 删除可以回滚7. DDL (Data Define Language)7.1 库的管理创建： CREATE DATABASE [IF NOT EXISTS]库名; 提高兼容性删除：DROP DATABASE [IF EXISTS] 库名更改库的字符集：ALTER DATABASE 库名 CHARACTER SET gbk7.2 表的管理创建表123456CREATE TABLE 表名( 列名 表的类型[(长度) 约束] 列名 表的类型[(长度) 约束] ... 列名 表的类型[(长度) 约束])修改表（修改整个表的结构，不是修改某一数据）修改列名：ALTER TABLE 表名 CHANGE [COLUMN] 列名 旧列名 新列名 新列名的类型修改列的类型：ALTER TABLE 表名 MODIFY CLOUMN 列名 新类型添加列：ALTER TABLE 表名 ADD COLUMN 列名 列类型删除列：ALTER TABLE 表名 DROP COLUMN 列名 列类型修改表名：ALTER TABLE 表名 RENAME TO 新表名删除表DROP TABLE [IF EXISTS] 表名复制表TODO:[E116]8. TCL (Transaction Control Language)9. SQL 常用函数9.1 单行函数对整个列操作，一列作为输入，一列作为输出9.1.1 字符函数LENGTH: 获取参数值的字节个数一个汉字占三个字节 (utf8) 两个字节 (gbk)，用 SHOW VARIABLES LIKE &#39;%char%&#39;CONTACT: 拼接字符串CONCAT(str1,str2,...) 用于字符的拼接，若字符串为 NULL 则返回 NULLUPPER, LOWER: 大小写SUBSTR: 截取满足某 字符长度 的子串SELECT SUBSTR(&#39;abcde&#39;,2); : [bcde]SELECT SUBSTR(&#39;abcde&#39;,1,3); : [abc]INSTR: INSTR(str,substr) ，返回 substr 在 str 中第一次出现的索引，没有返回 0TRIM: SELECT TRIM([remstr FROM] str) 去掉首尾的 remstr，默认空格LPAD:LPAD(str,n,c) 左填充，用指定字符 c 填充 str 左侧至 n 长度RPAD:RPAD(str,n,c) 右填充，用指定字符 c 填充 str 左侧至 n 长度REPLACE: REPLACE(str,a,b) 用 a 替换 bSQL 中索引从 1 开始9.1.2 数学函数ROUND: 四舍五入CEIL: 向上取整TRUNCATE: 截断MOD: 取余RAND: 0-1的随机数9.1.3 日期函数NOW: 返回当前系统日期 + 时间，精确到秒CURDATE: 返回当前日期，不包含时间CURTIME: 返回当前时间，不包含日期YEAR(日期字段): 返回字段的年MONTHNAME(日期字段): 返回字段的月的英文STR_TO_DATE: str_to_date(&#39;9-13-1999&#39;,&#39;%m-%d-%Y&#39;) 字符转换成日期格式DATE_FORMAT: DATE_FORMAT(&#39;2018/6/6&#39;,&#39;%Y年%m月%d日&#39;) 日期转换成格式字符DATEDIFF: 两个日期的天数差值格式解释9.1.4 其他函数IFNULL:判断字段表达式是否为 NULL，是的话返回 1，否则返回 0IFNULL:IFNULL(check_expression, replacement_value) 检查 check_expression是否为 NULL的表达式，如果不为 NULL 则保留，如果为 NULL 则返回 replacement_valueLENGTH: 字符长度9.1.5 流程控制函数IF: IF(exp1,exp2,exp3) 若 exp1 为true，返回 exp2，否则返回 exp3CASE: 也是对某一列进行操作，有两种情况：类似于 switch （等值判断）：123456CASE 要判断的字段或表达式WHEN 常量1 THEN 要显示的值 1 或者语句 1WHEN 常量2 THEN 要显示的值 2 或者语句 2...ELSE 要显示的值 n 或者语句 n （可以省略）END;类似于 if…else （区间判断）：123456CASE WHEN 条件1 THEN 要显示的值 1 或者语句 1WHEN 条件2 THEN 要显示的值 2 或者语句 2...ELSE 要显示的值 n 或者语句 n（可以省略）END;9.2 分组函数（统计函数）分组函数（为了分组查询使用），做统计使用，又称统计函数或聚合函数或组函数，一列作为输入，一个值作为输出SUM：求和AVG：平均MIN：最小值MAX：最大值COUNT：计算非 NULL 值得个数，用的最多SUM、AVG 一般用于处理数值型MAX、MIN、COUNT 可处理任何类型数据以上函数全部忽略 NULL和 DISTINCT 搭配，如 SUM(DISTINCT salary)：去重之后求和SELECT COUNT(*) FROM employees; 计算总行数（每一列不全为 NULL 的行数）效率比较高SELECT COUNT(1) FROM employees; 计算总行数SELECT COUNT(字段) FROM employees; 计算该字段非空值的个数和分组函数一同查询的字段要求是 group by 后的字段参考资料[1] MySQL 基础+高级篇- 数据库 -sql -尚硅谷, 李玉婷, bilibili.com, 2017 [视频链接]]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MATLAB 笔记]]></title>
    <url>%2Fposts%2F52233%2F</url>
    <content type="text"><![CDATA[与其他编程语言不同，MATLAB 是从 1 开始索引的，通过括号进行索引: A(1)一般来说有个 dim 参数，类似于 numpy 中 axis 参数，1 表示计算每列，2 表示计算每行，如 mean(A,1): 计算每一列的均值，返回一个行向量矩阵基本操作将矩阵 A 列向量化：A(:)将矩阵 A B 横着合并：[A B]将矩阵 A B 竖着合并：[A; B]所有列求和：sum(A,2)画图操作标记 x 轴：xlabel(&#39;time&#39;)标记 y 轴：ylabel(&#39;time&#39;)给 x 轴 y 轴修改坐标轴范围：axis([0.5 1 -1 1])（x 0.5 到 1，y -1 到 1）标记曲线：legend(&#39;sin&#39;, &#39;cos&#39;)保存文件：print -dpng &#39;myplot.png&#39;画多图在同一figure中：subplot(1,2,1)（生成 1*2 的布局，首先画第一张图）对矩阵画图观察矩阵分布大小：imagesc(A)（通常配合 colorbar colormap gray 生成带灰度 bar 的矩阵数值大小分布图）在当前画图状态下继续画图：hold on在新的画图状态下画图：hold off##]]></content>
      <categories>
        <category>软件技能</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶笔记]]></title>
    <url>%2Fposts%2F47671%2F</url>
    <content type="text"><![CDATA[“An interpreted, high-level, general-purpose programming language.”函数的参数1结论：Python 中一共有五种参数类型，分别是：位置参数（必选参数）、默认参数、可变参数、关键字参数和命名关键字参数，参数定义的顺序必须是：（位置参数）必选参数、默认参数、可变参数、命名关键字参数和关键字参数。必选参数（位置参数）指的是在给函数传参数时，按照顺序，依次传值。默认参数就是在写函数的时候直接给参数传默认的值，调用的时候，默认参数已经有值，就不用再传值了。默认参数可以简化函数的调用。设置默认参数时，有几点要注意：必选参数在前，默认参数在后，否则 Python 的解释器会报错，因为如果默认参数在前传递一个新的值，由于前面的默认参数有了默认值， Python 不知道到底是更新默认参数还是给必选参数。当函数有多个参数时，把 变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。默认参数必须指向不变对象！ 因为不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。我们在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。可变参数类与实例__str__和__repr__方法： __str__方法 print 该对象时会被调用，是给用户看的，__repr__直接输入对象查看该对象属性，是给开发看的1.廖雪峰的官方网站 ↩2.python3-cookbook ↩]]></content>
      <categories>
        <category>软件技能</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的数学基础]]></title>
    <url>%2Fposts%2F33591%2F</url>
    <content type="text"><![CDATA[文章目录1. 线性代数与矩阵分析1.1 可逆矩阵与奇异矩阵1.2 二次型与半正定矩阵1.3 特征值与特征向量1.4 矩阵微分1.4.1 Hessian 矩阵2. 数值计算与数学分析2.1 全微分与梯度2.2 梯度下降算法2.2.1 批量梯度下降(Batch Gradient Descent)2.2.2 随机梯度下降(Stochastic Gradient Descent)2.2.3 小批量梯度下降(Mini-Batch Gradient Descent)3. 优化理论3.1 拉格朗日乘子法(Lagrange multipliers)与 KKT 条件3.1.1 等式约束3.1.2 不等式约束与 KKT 条件3.1.3 拉格朗日对偶性(Lagrange Duality)与 Slater 条件[6]参考资料1. 线性代数与矩阵分析1.1 可逆矩阵与奇异矩阵逆矩阵（inverse matrix）: 给定一个 $n$ 阶方阵 $\mathbf {A}$，若存在一 $n$ 阶方阵 $\mathbf {B}$ ，使得 $\mathbf{AB}=\mathbf{BA}=\mathbf{I}_n$，则称 $\mathbf{A}$ 是可逆的，且 $\mathbf {B}$ 是 $\mathbf{A}$ 的逆矩阵，记作 $\mathbf {A} ^{-1}$。可逆矩阵叫做 非奇异矩阵（non-singular），在数学中，“奇异”（singular）一词用来形容破坏了某种优良性质的数学对象。对于矩阵来说，“可逆”是一个好的性质，不可逆的矩阵就称为“奇异”矩阵，可以这样按两方面简单理解，我们知道，如果一个 $n$ 阶方阵的列向量线性无关，那么这个矩阵可逆。如果一个矩阵不可逆，说明列向量线性相关，即某个列向量可以被其余列向量线性表示，而线性表示可以理解为按列向量排列的线性空间中的点具有某种“共低维（小于 $n$ 维度）空间的性质”（一个 $2\times 2$ 的矩阵不可逆表示列向量共线，一个 $3\times 3$ 方阵不可逆表示列向量共面），这样对于一个 $n$ 维空间的向量来说其实是非常 “奇异” 的，所以， 不可逆矩阵叫做一个奇异矩阵。我们也知道，若 $A$ 不可逆，则 $|A|=0$，材料中用“体积”方法直观的解释了行列式的几何意义：上图中阴影部分的面积表示了一个二阶矩阵的一个行列式的值，若这个矩阵不可逆，两个向量线性相关，说明两个向量共线，这也说明阴影部分的面积为 $0$，故 不可逆矩阵的行列式为 $0$，而二维空间中两个向量共线是非常“奇异的”，所以 不可逆矩阵叫做一个奇异矩阵。1.2 二次型与半正定矩阵对于一个方阵 $A \in \mathbb{R}^{n \times n}$ 和向量 $x \in \mathbb{R}^{n}$，标量 $x^{T} A x$ 叫做一个二次型按这种定义，则有：$$x^{T} A x=\sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} x_{i} x_{j}$$就相当于对矩阵 $A$ 所有元素都计算为二次项系数，由于对于只考虑 $i,j$ 位置的和，所以 $i,j$ 和 $j,i$ 的系数可以一分为二，故 $A$ 可以人为的构造成一个 对称矩阵，例如：$$\begin{pmatrix}x \\y\end{pmatrix}\begin{pmatrix}1 &amp; 2 \\3 &amp; 4 \\\end{pmatrix}\begin{pmatrix}x &amp; y\end{pmatrix} = x^2+5xy+4y^2 = \begin{pmatrix}x \\y \\\end{pmatrix}\begin{pmatrix}1 &amp; 5/2 \\5/2 &amp; 4 \\\end{pmatrix}\begin{pmatrix}x &amp; y\end{pmatrix}$$一个正定矩阵一定是一个可逆矩阵对于矩阵$A \in \mathbb{R}^{m \times n}$，矩阵 $G=A^{T} A$ 叫做 Gram matrix，而对于一个列满秩（“竖长的矩阵”）矩阵 $A$，$x^{T} A^{T}A x$ 是一个半正定二次型，而当 $A x=0$ 时二次型等于零，由于 $A$ 列满秩，故二次型等于零 $x$ 无解，故 Gram matrix 是个正定矩阵，即是一个可逆矩阵。一般来说把 $A$ 看做训练集，训练集中样本数远大于特征数，所以 $A$ 一般是个列满秩矩阵.1.3 特征值与特征向量给定一个 $A \in \mathbb{R}^{n \times n}$，$$A x=\lambda x, \quad x \neq 0$$ 则称 $\lambda \in \mathbb{C}$ 为矩阵 $A$ 的特征值，$x \in \mathbb{C}^{n}$ 为对应特征值的特征向量.在技术上，我们一般是通过计算 $|(\lambda I-A)|=0$ 找到其特征值与特征向量，关于特征值与特征向量，有以下一些性质：矩阵的迹等于特征向量之和：$\operatorname{tr} A=\sum_{i=1}^{n} \lambda_{i}$矩阵的行列式等于迹的乘积：$|A|=\prod_{i=1}^{n} \lambda_{i}$对角矩阵 $D=\operatorname{diag}\left(d_{1}, \ldots d_{n}\right)$ 的特征值为 $d_{1}, \ldots d_{n}$.对称矩阵的所有特征值都为实数，且其特征向量标准正交我们可以把所有的特征向量写在同一个矩阵 $X$ 中，可以写成：可以得到：$$A X=X \Lambda$$，若 $A$ 是对称矩阵，则 $U$ 是一个正交阵，$A=U \Lambda U^{T}$，可以得到：$$ x^{T} A x=x^{T} U \Lambda U^{T} x=y^{T} \Lambda y=\sum_{i=1}^{n} \lambda_{i} y_{i}^{2}$$而这一步就相当于二次型在配方，所以二次型的正定型取决于对称矩阵的特征值.1.4 矩阵微分定义 $f : \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$ 是一个矩阵到实数的一个映射，$$\nabla_{A} f(A) \in \mathbb{R}^{m \times n}=\left[ \begin{array}{cccc}{\frac{\partial f(A)}{\partial A_{11}}} &amp; {\frac{\partial f(A)}{\partial A_{12}}} &amp; {\cdots} &amp; {\frac{\partial f(A)}{\partial A_{1}}} \\ {\frac{\partial f(A)}{\partial A_{21}}} &amp; {\frac{\partial f(A)}{\partial A_{22}}} &amp; {\cdots} &amp; {\frac{\partial f(A)}{\partial A_{2 n}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial f(A)}{\partial A_{m 1}}} &amp; {\frac{\partial f(A)}{\partial A_{m 2}}} &amp; {\cdots} &amp; {\frac{\partial f(A)}{\partial A_{m n}}}\end{array}\right]$$对于一个一个标量求梯度输出也是一个矩阵，其维数应该等于其输入的维数，所以到底谁是输入的矩阵是非常重要的，例如对于系数矩阵$A$ 和一个实向量变元$x$，对于 $\nabla f(A x)$ 可以认为输入的维数是和 $Ax$ 相同，所以输出的梯度不是一个与 $x$ 维度相等的向量，也可以认为输入的矩阵是 $x$，所以输出应该是一个矩阵，这两种方式理解都正确，类似于微积分中 $f(ax)$ 的 导数一样，要明确对谁求导数，可以这样规定：如果有下标，则对下标内的矩阵求梯度，即 $\nabla_{x} f(A x)$ 对 $x$ 求梯度，输出一个向量.若没有下标，默认对括号内求梯度，即 $\nabla f(A x)$ 输出一个维度与 $Ax$ 相等的矩阵.1.4.1 Hessian 矩阵与梯度类似，$$\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}=\left[ \begin{array}{cccc}{\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f(x)}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{n}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{n}^{2}}}\end{array}\right]$$这个矩阵是一个对称矩阵，不能看做梯度的梯度，因为梯度本身就是一个向量，向量没法求其梯度，几个有用的结论：$\nabla_{x} b^{T} x=b$$\nabla_{x} x^{T} A x=2 A x$ 若 $A$ 是对称矩阵，特别地，$\nabla_{x} x^{T}x=2x$$\nabla_{x} x^{T} A x=(A^{T}+A) x$ 若 $A$ 是非对称矩阵$\nabla_{x}^{2} x^{T} A x=2 A$ 若 $A$ 是对称矩阵$\nabla_{A}|A|=(\operatorname{adj}(A))^{T}=|A| A^{-T}$$\nabla_{A} \log |A|=\frac{1}{|A|} \nabla_{A}|A|=A^{-1}$(用到了链式法则)$\nabla_{A} \operatorname{tr} A B=B^{T}$$\nabla_{A^{T}} f(A)=\left(\nabla_{A} f(A)\right)^{T}$$\nabla_{A} \operatorname{tr} A B A^{T} C=C A B+C^{T} A B^{T}$2. 数值计算与数学分析2.1 全微分与梯度梯度(gradient)这个概念在整个机器学习和神经网络中占据重要地位，是非常重要的基础概念之一，本小节尽量利用通俗的语言去解释梯度这个概念，在理解梯度之前我们先回顾一下全微分全微分，是多变数微积分的一个概念基本上就代表多元函数的微分，多变量函数在某点的全微分为一线性映射，通常可用矩阵或向量表示. 全微分可以看成是把单变数函数的微分推广到多变数函数上，其意义为多元函数变化量的线性逼近。例如，对于二元函数有：$$\boxed{d z=f_{x} d x+f_{y} d y=\frac{\partial z}{\partial x} d x+\frac{\partial z}{\partial y} dy }$$初学者比较难理解这个公式怎么来的，此公式的主要思想是某点附近非常小的曲面用平面代替，如下图所示，红色平面为切平面，$A$ 点坐标为 $(x,y)$，$B$ 点坐标为 $(x+\Delta x,y+\Delta y)$，现在要比较这两点函数值 $f(x+\Delta x,y+\Delta y)-f(x,y)$ 的差值 $dz$显然路线应该是切平面的线段 $DF$，若我们考虑 $DF$ 在 $XOZ$ 与 $YOZ$ 平面的投影 $DE$ 和 $EF$，则我们可以通过计算路径 $D\rightarrow E\rightarrow F$ 来计算 $dz$，而 $\tan{\alpha},\tan{\beta}$ 正好刻画了两个方向的偏导数，所以有$$dz = |EG|+|FH| = \frac{\partial f}{\partial x}\Delta x+\frac{\partial f}{\partial y}\Delta y$$不难想象，如果拓展到更高维空间，也是这种积和的形式，这种形式恰好可以理解为向量内积，把 $J:=(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})$ 定义为梯度，站在曲面上的点为 $C$, 而他所能前进的方向($XOY$平面上)为 ($\Delta x,\Delta y)$，我们只考虑方向的话，不妨设为$(\cos{\gamma},\sin{\gamma})$，由柯西不等式，$C$点在做了此决策之后能最高能够上升的高度为$$\frac{\partial f}{\partial x}\cos{\gamma}+\frac{\partial f}{\partial y}\sin{\gamma}\leq \sqrt{(\frac{\partial f}{\partial x})^2+(\frac{\partial f}{\partial y})^2}$$当且仅当$(\Delta x,\Delta y)=J$ 时成立，也就是说，决策者 $C$ 沿着梯度的方向走，上升最快，梯度是函数值上升的最快的方向（在投影平面上）梯度的方向是函数值上升的最快的方向 (在自变量的平面内)梯度的大小决定函数值上升的速率 ($dz$)其实这里还蕴藏着一个几何性质，我们知道利用平面去代替这个附近的曲面，妙就妙在这个平面用分量的偏微分就可以完全刻画了，实际上，平面很重要的一个性质就是法向量，而法向量其实也可以用这两个偏微分表示：$$\boldsymbol{n} = (\frac{\partial f}{\partial x},\frac{\partial f}{\partial y},-1)$$这就是 $z=f(x,y)$ 在点 $(x,y)$ 处的切平面法向量，证明也是简单的：在上图中我们把 $D,E,F$ 三点坐标表示出来，可以求得向量 $\vec{DE} = \Delta x(1,0,\frac{\partial f}{\partial x})$，$\vec{EF} = \Delta y(1,0,\frac{\partial f}{\partial y})$，则 $\boldsymbol{n}\bot\vec{EF}, \vec{DE}$. 而原曲面的方程为 $g(x,y,z)=f(x,y)-z=0$ 是 $g(x,y,z)$ 分别对 $x,y,z$ 求偏导之后的结果，是一个四维空间的梯度，我们将这个结论拓展到高维空间则有：曲面 $g(\boldsymbol{x})=0$ 上任意一点 $\boldsymbol{x}$，该点梯度 $\nabla g(\boldsymbol{x})$ 正交于该曲面 ($\boldsymbol{x}$ 为向量)关于曲面和函数这两个概念有点让人混淆，现在我们可以对梯度做一个总结：设 $\boldsymbol{x}$ 是一个 $d$ 维向量，$f$ 是一个$\mathbb{R}^{d} \rightarrow \mathbb{R}$ 的映射，则有以下结论：对于 $y=f(\boldsymbol{x})$ 来说是一个函数，几何上表示的是一个 $d+1$ 维度空间的曲面（流形）（例如 $z=f(x,y)=x+y$ 实际上是一个三维平面），对这个函数对每个分量求偏导数，得到一个 $d$ 维空间的向量 $\nabla f(\boldsymbol{x})$，向量的方向表示在 $d$ 维空间上看 $f$ 上升最快的方向，分量大小表示上升速率；$f(\boldsymbol{x})=0$ 是一个 $d$ 维的曲面，是 $y=f(\boldsymbol{x})$ 这个 $d+1$ 维流形的等值面，梯度 $\nabla f(\boldsymbol{x})$ 是这个 $d$ 维曲面的法向量.2.2 梯度下降算法2.2.1 批量梯度下降(Batch Gradient Descent)2.2.2 随机梯度下降(Stochastic Gradient Descent)2.2.3 小批量梯度下降(Mini-Batch Gradient Descent)3. 优化理论3.1 拉格朗日乘子法(Lagrange multipliers)与 KKT 条件拉格朗日乘子法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法. 这种方法可以将一个有 $d$ 个变量与 $k$ 个约束条件的最优化问题转换为一个 $d+k$ 个变量的无约束优化问题求解.3.1.1 等式约束我们先考虑如下只带一个约束条件的优化问题：$$\begin{aligned}&amp;\min_{\boldsymbol{x}}f(\boldsymbol{x})\\&amp;\text{ s.t. }g(\boldsymbol{x})=0\end{aligned}\tag{3.1}$$其中 $\boldsymbol{x}$ 为 $d$ 维向量，那么 $f(\boldsymbol{x})$ 为 $d+1$ 维空间的曲面，$g(\boldsymbol{x})=0$ 为 $d$ 维空间的曲面. 设 $\boldsymbol{x}^{*}$ 是 最优点（自由度其实是$d$），我们有以下结论：约束曲面 $g(\boldsymbol{x})=0$ 上任意一点 $\boldsymbol{x}$，该点梯度 $\nabla g(\boldsymbol{x})$ 正交于该约束曲面 (2.1 小节)在最优点 $x^{*}$ 处的目标函数 $f(\boldsymbol{x})$ 的梯度 $\nabla f(\boldsymbol{x}^{*})$正交于约束曲面如果我们吧 $g(\boldsymbol{x})=0$ 想象成三维空间的曲面的话不好理解，因为此时 $f(x)$ 就是四维流形了 (2.1 节)，假设 $g(\boldsymbol{x})=0$ 是一个二维曲线，如下图所示如果在最优点 $x^{*}$ 处的目标函数 $f(\boldsymbol{x})$ 的梯度 $\nabla f(\boldsymbol{x}^{*})$与约束曲面不正交，说明在 $\boldsymbol{x}^{*}$ 处约束曲面上还可以沿着某个分量的其他方向使得 $f(\boldsymbol{x})$ 达到更大（梯度是 $d$ 维空间的向量，$f(\boldsymbol{x})=0$ 只是 $d$ 维空间的一部分，梯度完全有可能不在 $g(\boldsymbol{x})=0$ 内），所以一定与约束平面正交，但与 $\nabla f(\boldsymbol{x})$ 方向不一定一致，所以存在常数 $\lambda$ ，$f(\boldsymbol{x})$ 在 $\boldsymbol{x}^{*}$ 取到极值时有：$$\nabla f\left(\boldsymbol{x}^{*}\right)+\lambda \nabla g\left(\boldsymbol{x}^{*}\right)=0\tag{3.2}$$我们发现一个 trick：$$L(\boldsymbol{x}, \lambda)=f(\boldsymbol{x})+\lambda g(\boldsymbol{x})\tag{3.3}$$上式称为拉格朗日函数，对 $\boldsymbol{x}$ 的偏导数就是 (3.2)，同时这个函数对 $\lambda$ 的偏导数正好就是约束条件，那么我们就可以把这个等式约束融合在目标函数里而变成无约束的优化问题.3.1.2 不等式约束与 KKT 条件不等式约束稍微比等式约束稍微复杂一点，我们考虑两种情况：$f(\boldsymbol{x})$ 在边界上取到极值，此时对应等式约束的情况，即 $g(\boldsymbol{x}^{*})=0$$f(\boldsymbol{x})$ 在内部取到极值，此时对应 $g(\boldsymbol{x}^{*})&lt;0$ 的情况对于情况一，我们回到了 (3.2) 式，但注意，此时的 $\lambda&gt;0$，这是因为： $\nabla g(\boldsymbol{x})$ 是 $y=g(\boldsymbol{x})$ 上升最快的方向，相对于此时 $\nabla g(\boldsymbol{x})=0$ 来说，肯定指向 $g(\boldsymbol{x})&gt;0$ 的方向，所以指向外部；而我们现在求的是 $f$ 的极小值，内部的点比外部要大，$\nabla g(\boldsymbol{x})$ 肯定指向内部，所以 $\lambda&gt;0$，这时候只需要$$\begin{aligned}&amp;\nabla f\left(\boldsymbol{x}^{*}\right)+\lambda \nabla g\left(\boldsymbol{x}^{*}\right)=0\\&amp;g(\boldsymbol{x})=0\\&amp;\lambda&gt;0\end{aligned}\tag{3.4}$$对于情况二，解在内部约束条件就是无效的，因为在内部的解不管朝什么方向移动一小步都仍然满足约束条件，永远达不到边界，所以只需要 $\nabla f(\boldsymbol{x})=0$ 即可，这等价于(3.2)式 $\lambda=0$，将两种情况一合并在一起得到 $\lambda g(\boldsymbol{x})=0$$$\left\lbrace\begin{array}{l}{g(\boldsymbol{x}) \leqslant 0} \\{\lambda \geqslant 0} \quad \quad \quad \quad \text{对偶可行性}\\{\lambda g(\boldsymbol{x})=0} \quad \quad \text{互补松弛性}\end{array}\right.\tag{3.5}$$上式称为 KKT 条件(Karush–Kuhn–Tucker conditions).上面两种情况可以用上面这张图片表示，图为函数的等值线与约束曲线 $g(\boldsymbol{x})$ 的关系，当 $\boldsymbol{x}^{*}$ 在内部时满足 $\nabla f(\boldsymbol{x})=0$，当 $\boldsymbol{x}^{*}$ 在外部时当且仅当边界与等值线相切的时候取到最大值，因为等高线始终与 $\nabla f(\boldsymbol{x})$ 正交.(3.5) 的结果可推广至多个约束等式与约束不等式的情况。考虑标准约束优化问题$$\begin{array}{ll}{\min} &amp; {f(\boldsymbol{x})} \\{\text { s.t. }} &amp; {h_{i}(\boldsymbol{x})=0 \quad(i=1, \ldots, m)} \\{} &amp; {g_{j}(\boldsymbol{x}) \leqslant 0 \quad(j=1, \ldots, p)}\end{array}\tag{3.6}$$引入拉格朗日乘子 $\boldsymbol{\lambda}$，$\boldsymbol{\mu}$$$L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=f(\boldsymbol{x})+\sum_{i=1}^{m} \lambda_{i} h_{i}(\boldsymbol{x})+\sum_{j=1}^{p} \mu_{j} g_{j}(\boldsymbol{x})\tag{3.7}$$由不等式引入的 KKT 条件为：$$\left\lbrace\begin{array}{l}{g_{j}(\boldsymbol{x}) \leqslant 0} \\{\mu_{j} \geqslant 0}\\{\mu_{j} g_{j}(\boldsymbol{x})=0}\end{array}\right.\tag{3.8}$$3.1.3 拉格朗日对偶性(Lagrange Duality)与 Slater 条件[6]我们重新考虑 (3.6) 这个式子，任意一个带约束的优化都可以写成这样的形式（若求最大可以在目标函数前加负号转化为求最小）. 若 $h_1,h_2,\ldots,h_m$ 和 $f$ 都是凸函数 ，并且 $g_1,g_2,\ldots,g_p$ 全都是仿射函数（就是形如 $Ax+b$ 的形式），那么这个问题就叫做凸优化(Convex optimization)问题. 凸优化问题有许多优良的性质，例如它的极值是唯一的。不过，这里我们并没有假定需要处理的优化问题是一个凸优化问题. 对于 (3.7) 我们假设$$z(\boldsymbol{x})=\max_{\boldsymbol{\mu}\succeq 0, \boldsymbol{\lambda}}L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})\tag{3.9}$$这里 $\boldsymbol{\mu}\succeq 0$ 理解为向量 $\boldsymbol{\lambda}$ 的每一个元素都非负即可，上式是一个关于 $\boldsymbol{x}$ 的函数，我们有以下结论，当 $\boldsymbol{x}$ 满足(3.6)的约束时，则有$$f(\boldsymbol{x})=z(\boldsymbol{x})\tag{3.10}$$ 这也是容易验证的，满足(3.6)的约束时，$h(\boldsymbol{x}_{i})=0$，注意到 $\mu_{j}g(\boldsymbol{x}_{j})$ 的非正性，最大化 $z(x)$ 时显然 $\mu_{i}g(\boldsymbol{x}_{j})=0$，所以就证明了上面的式子，这样一来，原始的带约束的优化问题(3.6)其实等价于如下的无约束优化问题：$$\min_{\boldsymbol{x}}z(\boldsymbol{x})=\min_{\boldsymbol{x}}\max_{\boldsymbol{\mu}\succeq 0, \boldsymbol{\lambda}}L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})\tag{3.11}$$ 我们可以把 $\boldsymbol{x}$ 所在的整个空间分为两部分：一部分是满足约束条件 $P_1$，另一部分是不满足约束条件的 $P_2$：当 $\boldsymbol{x} \in P_1$ 时由 3.10 可知 $\min_{\boldsymbol{x}}f(\boldsymbol{x}) = \min_{\boldsymbol{x}}z(\boldsymbol{x})$当 $\boldsymbol{x} \in P_2$ 时，$\max_{\boldsymbol{\mu}\succeq 0, \boldsymbol{\lambda}}L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=\max_{\boldsymbol{\mu}\succeq 0, \boldsymbol{\lambda}}(f(\boldsymbol{x})+\infty)$，因为 $h$ 和 $g$ 在约束条件之外 $\lambda$ 和 $\mu$ 总能改变其值使得 $\sum_{i=1}^{m} \lambda_{i} h_{i}(\boldsymbol{x})+\sum_{j=1}^{p} \mu_{j} g_{j}(\boldsymbol{x})=+\infty$综合这两点，考虑$\boldsymbol{x}$ 的整个空间上要达到最小，$P_2$ 自动就被排除在外，所以最小化 $z(\boldsymbol{x})$ 就变成了目标函数为(3.11)的无约束规划（这里无约束是相对 $\boldsymbol{x}$ 来说，$\boldsymbol{\mu}$ 还是有约束）. 而这样写只是对原始式子做了一个变换，把约束条件融合在目标函数里，原问题没有发生本质变化，我们把 (3.11) 无约束目标规划成为原问题(primal problem)，是最小化 $\boldsymbol{x}$. 相对应的还有一个对偶问题(dual problem)，其形式非常类似，只是把 $\min$ 和 $\max$ 交换了一下：$$\max_{\boldsymbol{\mu}\succeq 0, \boldsymbol{\lambda}}g(\boldsymbol{\mu},\boldsymbol{\lambda})=\max_{\boldsymbol{\mu}\succeq 0, \boldsymbol{\lambda}}\min_{\boldsymbol{x}}L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})\tag{3.12}$$ 交换之后的 dual problem 在最大化 $\boldsymbol{\mu},\boldsymbol{\lambda}$， 这和原来的 primal problem 并不相等，直观地，我们可以这样来理解：中国乒乓球国家队最差的选手比国外乒乓球最好的选手要强. 换句话说，如果 primal problem 的最小值记为 $p^{*}$，dual problem 的最大值记为 $d^{*}$，则有$$d^{*}\leqslant p^{*}\tag{3.13}$$这个性质叫做 weak duality ，对于所有的优化问题都成立。其中 $p^{*}−d^{*}$ 被称作 duality gap. 需要注意的是，无论 primal problem 是什么形式，dual problem 总是一个 convex optimization 的问题[7]：$$\begin{array}{ll}{\max} &amp; g(\boldsymbol{\mu},\boldsymbol{\lambda}) \\{\text { s.t. }} &amp; \boldsymbol{\mu}\succeq 0\end{array}\tag{3.14}$$ 它的极值是唯一的（如果存在的话），并且有现成的软件包可以对凸优化问题进行求解（虽然求解 general 的 convex optimization 实际上是很慢并且只能求解规模较小的问题的）。这样一来，对于那些难以求解的 primal problem （比如，甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。或者说我们甚至都不用去优化这个 dual problem ，而是（通过某些方法，例如随机）选取一些 $\lambda\geqslant 0$ 和 $\mu$ ，带到 $g(\boldsymbol{\mu},\boldsymbol{\lambda})$ 中，这样也会得到一些下界（只不过不一定是最大的那个下界而已）。当然要选 $\boldsymbol{\mu}$ 和 $\boldsymbol{\lambda}$ 也并不是总是“随机选”那么容易，根据具体问题，有时候选出来的 $\boldsymbol{\lambda}$ 和 $\boldsymbol{\mu}$ 带入 $g$ 会得到 $−\infty$ ，这虽然是一个完全合法的下界，然而却并没有给我们带来任何有用的信息.故事到这里还没有结束，既然有 weak duality ，显然就会有 strong duality. 所谓 strong duality ，就是$$d^{*}\leqslant p^{*}\tag{3.14}$$这是一个很好的性质，strong duality 成立的情况下，我们可以通过求解 dual problem 来优化 primal problem ，在 SVM 中我们就是这样做的。当然并不是所有的问题都能满足 strong duality ，在讲 SVM 的时候我们直接假定了 strong duality 的成立，这里我们就来提一下 strong duality 成立的条件。不过，这个问题是一个很复杂的问题，这里直接给出结论：Slater 条件Slater 条件是指存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $g_{i}(x)\leqslant 0$ 中的“小于或等于号”要严格取到“小于号”，亦即，存在 $x$ 满足:$$\begin{array}{ll}{g_{i}(x)&lt;0} &amp; {i=1, \ldots, m} \\{h_{i}(x)=0} &amp; {i=1, \ldots, p}\end{array}\tag{3.15}$$如果原始问题是 Convex 的并且满足 Slater 条件的话，那么 strong duality 成立需要注意的是，这里只是指出了 strong duality 成立的一种情况，而并不是唯一情况。例如，对于某些非 convex optimization 的问题，strong duality 也成立. 在 SVM 的 primal problem 中是一个 QP（QP 是凸优化问题的一种特殊情况），而 Slater 条件实际上在这里就等价于是存在这样的一个超平面将数据分隔开来，亦即是“数据是可分的”. 所以在 Slater 条件成立的情况下用对偶问题可以求出极值点.对偶问题和 KKT 条件还有一个定理，即若对偶问题和原问题有相同的解，则满足 KKT 条件（$f,g$ 为凸，$h$ 为仿射函数的情况下）. 我们结合 Slater 定理可以得到若 $f,g$ 为凸，$h$ 为仿射函数，$x$ 和 $\mu,\lambda$ 分别是原问题和对偶问题的解存在严格小于$\Longleftrightarrow$ $x$ 和 $\mu,\lambda$ 满足 KKT 条件(3.8)TODO:文档过长要拆分参考资料[1] Mathematics For Machine Learning, Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong, Cambridge University Press, 2020 [PDF][2] Linear Algebra Review and Reference, Zico Kolter, CS224 material, 2015 [PDF][3] 矩阵分析与应用, 张贤达, 清华大学出版社, 第二版, 2013 [PDF][4] Mathematics for Data Science, Ibrahim Sharaf ElDen, towardsdatascience.com, 2019 [Link][5] 机器学习 (附录), 周志华，清华大学出版社, 第一版, 2016 [Link][6] 支持向量机：Duality, 张驰原, pluskid.org, 2010, [Link][7] Stephen Boyd, et al. Convex Optimization, Cambridge university press, 2004. [Link]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数学</tag>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性模型]]></title>
    <url>%2Fposts%2F41333%2F</url>
    <content type="text"><![CDATA[“A broad introduction to machine learning and statistical pattern recognition”文章目录1. 机器学习的定义2. 训练样本与符号说明3. 线性回归1. 机器学习的定义在学习的过程中，学习算法试图通过训练数据集中的样本产生一个可以预测 $y$ 的预测系统，这个过程就叫做机器学习。2. 训练样本与符号说明规定 $X$ 表示训练集，$y$ 表示标签（一般大写字母表示矩阵，小写字母表示向量）$m$ = # training exmples.(训练样本个数，表的行数)$n$ = # features.(特征个数，表的列数)$(x,y)$ = “training example”$(x^{(i)},y^{(i)})$ = 第 $i$ 个训练样本. (表的第 $i$ 行)线性模型试图通过特征的线性组合来进行预测，即$$f(\boldsymbol{x})=w_0 + w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{n} x_{n}$$$w_0$ 叫做截距（intercept）如果不加上 $w_0$，当所有特征取 0 的时候训练集的标签一定为 0，这种假设显然是不合理的，为了统一写成样本的线性组合形式，我们人为地增加一维特征写成 $x_0=1$，所以上式写成：$$f(\boldsymbol{x})=w_0x_0 + w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{n} x_{n}\ \ \ (x_0=1)$$写成矩阵形式为：$$X=\left[ \begin{array}{c}{1 \ —\left(x^{(1)}\right)^{T}}— \\ {1 \ —\left(x^{(2)}\right)^{T}—} \\ {\vdots} \\ {1 \ —\left(x^{(m)}\right)^{T}–}\end{array}\right]_{m\times (n+1)}$$一般 $X$ 是一个 $m\times (n+1)$ 的矩阵，$n+1$ 维的特征，$m$ 个训练样本，而我们样本所对应的标签 $y$ 为：$$y=\left[ \begin{array}{c}{y^{(1)}} \\ {y^{(2)}} \\ {\vdots} \\ {y^{(m)}}\end{array}\right]$$我们的目的就是找到一个参数向量 $\theta = \left[ \theta_0,\theta_1,\cdots\theta_n\right]^{T}$，使得我们估计值 $\boldsymbol{f}(X)$：$$\boldsymbol{f}(X)=\left[ \begin{array}{c}{1 \ —\left(x^{(1)}\right)^{T}}— \\ {1 \ —\left(x^{(2)}\right)^{T}—} \\ {\vdots} \\ {1 \ —\left(x^{(m)}\right)^{T}–}\end{array}\right]\cdot\left[ \begin{array}{c}{\theta_0 \\ \theta_1 \\ {\vdots} \\ \theta_n}\end{array}\right]=X\theta$$与我们样本中的 $y$ 最接近，s3. 线性回归对于线性回归，我们假设参数和输入的特征服从线性函数，也就是说$$h(x)=\sum_{i=0}^{n} \theta_{i} x_{i}=\theta^{T} x$$这里假定 $x_0=1$，定义其损失函数为偏离输出变量的平方和，也就是$$J(\theta)=\frac{1}{2} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>CS229</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NetworkX 笔记]]></title>
    <url>%2Fposts%2F53355%2F</url>
    <content type="text"><![CDATA[“A Python package for complex networks”当前 Networkx 版本为 0.24一、网络的基本属性1.1 网络的类型1.2 连通性：判断是否连通：nx.is_connected(G)取网络中最大连通子图：Gc = G.subgraph(max(nx.connected_components(G), key=len)).copy()1.3 给网络、节点、连边增加属性给整个网络增加属性 ：在网络生成时给网络增加属性：G = nx.Graph(day=&quot;Friday&quot;)在网络生成之后修改网络属性：G.graph[&#39;day&#39;] = &quot;Monday&quot;显示网络属性：G.graph给点增加属性 ：在网络生成时添加点属性：G.add_node(1, time=&#39;5pm&#39;)、G.add_nodes_from([3], time=&#39;2pm&#39;)(因为有一个关键字参数 **attr)在网络生成之后修改点属性：G.nodes[1][&#39;room&#39;] = 714显示点属性：G.nodes.data()网络的度：G.degree()，如果无参数则返回所有节点的名称与其度值（DegreeView），若参数为一个节点名称，则返回改节点的度，若参数为节点序列，则返回节点名称与节点的度[d for n, d in G.degree()]：返回所有节点的度值二、二分图由于 NetworkX 没有一个自定义的二分图类型，所以所有的二分图都是 Graph() 或者 DiGraph() 类型，详细内容可参考 Bipartite检测图的二分性：networkx.is_bipartite()分割二分图：bottom_nodes, top_nodes = bipartite.sets(B)只有网络连通时分割结果才没有歧义，若网络不连通，则应该根据节点属性进行分割：top_nodes = {n for n, d in B.nodes(data=True) if d[&#39;bipartite&#39;]==0}bottom_nodes = set(B) - top_nodes投影（projection） ：投影为 Graph 或 MultiGraph：projected_graph(B, nodes, multigraph=False) 只考虑在 nodes 上构造的网络，若某两个点在 B 中有相同的邻居，则投影图中相邻，若有多个相邻的点，multigraph 参数决定了是否构造为 MultiGraph.（B 不需要要求为二分图，任意图都可以进行投影）子图按节点选取网络子图：G.subgraph(nodes)TODO:]]></content>
      <categories>
        <category>软件技能</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NetworkX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas 常用核心函数与方法]]></title>
    <url>%2Fposts%2F55796%2F</url>
    <content type="text"><![CDATA[“A powerful Python data analysis toolkit”当前 Pandas 版本为 0.24.1文章目录数据选取、索引、采集数据修改、删除、映射数据合并、连接、关系Concat 与 Append 操作Merge 操作Group By 技术分割应用累计与求和转换与应用拾遗axis 取 0 还是 1？正则表达式字符串时间序列操作Kaggle 数据竞赛常用命令数据选取、索引、采集.loc：利用索引定位，必须输入索引，输入索引不存在时会报错.iloc：i 表示 integer position，利用整数位置进行定位若用 df.loc、 df.iloc 或者 df[&#39;col&#39;] 选取子集，是选取了原来 dataframe 的视图而不是副本，修改这个子集会修改原来的 dataframe ，所以要选取副本，再加上 .copy() 可创建副本，有时候 Pandas 会抛出 SettingWithCopyWarning 的警告，绝大部分是因为副本视图问题造成的。选取多行：df.loc[[index_name1,index_name2]]选取多列：df[[col_name1, col_name2]]正则表达式选取：df[&#39;new_col&#39;] = df[&#39;col&#39;].str.extract(&#39;regex&#39;) 按照 regex 正则表达式选取 col 列中每一元素，结果保存在 new_col ，查看更多请点击 pandas.Series.str.extract正则表达式替换文本：df[&#39;col&#39;].str.replace(&#39;text&#39;,&#39;&#39;)（将 text 文本替换为空文本，即删除特定的字符），关于字符串操作，可以参考 python3-cookbook查找缺失数据：df.isnull()：对整个 df 判断是否为空，返回一张大表，这张表每个元素为布尔值，True 则为空值.判断某一列（行）是否存在缺失值：df.loc[&#39;index&#39;].isnull().any()，存在则返回 True，不存在则返回 False.对于数据很大的表，用 df.isnull() 不方便看到缺失值的具体位置，配合 pandas.Series.any() 函数可以确定某一行或者某一列出现缺失值，（axis=0: index,axis=1: columns），any() 方法表示判断给定的 Series 是否全部为 False ，若全为 False 则返回 False，如果有一个为 True，则返回 True，所以当返回 True 表示这一行或者这一列有缺失值判断某行（列）是否存在缺失数据：df.loc[&#39;index&#39;].isnull().any()行循环：for index, row in df.iterrows():数据修改、删除、映射生成一个数据表：df = pd.DataFrame(data={&#39;col1&#39;:[1,2,3],&#39;col2&#39;:[4,5,6]})增加一行：df.loc[&#39;new_raw&#39;] = &#39;1&#39;增加一列：df[&#39;new_colu&#39;]=&#39;1&#39;.按上面两种方式选取的子集为原来 df 的视图.删除一行：df.drop(&#39;raw_name&#39;, axis=0, inplace=True)，这里默认 drop 返回原 df 的视图，若设置 inplace=True 则修改 df 返回 None.删除一列：df.drop(&#39;raw_name&#39;, axis=1, inplace=True)删除缺失数据：df.dropna(axis=0, how=&#39;any&#39;, inplace=False, subset=[&#39;col&#39;]). axis=0 表示检查这一行，按照 how=&#39;any&#39; (这一行只要出现了缺失，就删除)的方法删除这一行，subset=[&#39;col&#39;] 表示只检查 col 这一列是否有缺失值，inplace=False 表示删除后不替换原来的变量.修改列(行)名称：df.rename(columns={&#39;old_col_name&#39;: &#39;new_col_name&#39;}, index={&#39;old_index_name&#39;: &#39;new_index_name&#39;},inplace=True)修改索引列名称：上面的方法无法改变索引列的名称，可以用 df.index.name = &#39;new_name&#39; 的方法.凡是输入后返回一张表，说明原来带有 inplace 参数，如需替换则设置为 True修改索引：reset_index() ，也可以将 MultiIndex 转换成普通的 Index.对索引排序：df.sort_index()，默认返回排序后的 df，可设置 inplace 参数.二级索引变成索引和列：df.unstack()，一般用在对两列的 groupby，之后转化为一级索引的 DataFrame，详情点击数据合并、连接、关系Concat 与 Append 操作Merge 操作Group By 技术Pandas 提供了一个灵活高效的 groupby 功能，可以对某些标签或索引的局部进行累计分析，很多复杂的操作都可以化为 GroupBy 操作(不能保证是效率最高的).手册上对于 GroupBy 主要可以分为以下三个步骤：分割-应用-组合Splitting the data into groups based on some criteria.Applying a function to each group independently.Combining the results into a data structure.上图表现了这种过程，而中间的分割过程不需要显式地暴露出来，这一点十分重要。而且 GroupBy(经常)只需要一行代码， 就可以计算每组的和、均值、计数、最小值以及其他累计值。GroupBy 的用处就是将这些步骤进行抽象: 用户不需要知道在底层如何计算，只要把操作看成一个整体就够了.分割In [1]: df = pd.DataFrame(&#123;'X': ['A', 'B', 'A', 'B'], 'Y': [1, 4, 3, 2]&#125;)In [2]: dfOut[2]: X Y0 A 11 B 42 A 33 B 2In [3]: df.groupby(['X'])Out[3]: &lt;pandas.core.groupby.groupby.DataFrameGroupBy object at 0x1144ea198&gt;可以看到，这里的返回值不是一个 DataFrame 对象，而是一个 DataFrameGroupBy 对象。 这个对象的魔力在于，你可以将它看成是一种特殊形式的 DataFrame，里面隐藏着若干组 数据，但是在没有应用累计函数之前不会计算。GroupBy 对象是一种非常灵活的抽象类型。在大多数场景中，可以将它看成是 DataFrame 的集合.默认情况下，GroupBy 对象会对 group keys 进行排序，如果分的组过多，可以设置 groupby([&#39;X&#39;], sort=False) 提高运行速度.默认情况下对多列进行 GroupBy 会产生 MultiIndex，可以设置 as_index=False 取消生成 MultiIndex，也可以用 .reset_index() 来实现.利用 get_group(&#39;group_name&#39;) 方法获得某一分组的 DataFrame，还可以利用 .groups 属性获得包含所有分组的字典，字典的键为组名，例如：In [27]: df.groupby('A').groupsOut[27]: &#123;'bar': Int64Index([1, 3, 5], dtype='int64'), 'foo': Int64Index([0, 2, 4, 6, 7], dtype='int64')&#125;这个属性可以用于组的循环：In [57]: grouped = df.groupby('A')In [58]: for name, group in grouped: ....: print(name) ....: print(group) ....: bar A B C D1 bar one 0.254161 1.5117633 bar three 0.215897 -0.9905825 bar two -0.077118 1.211526foo A B C D0 foo one -0.575247 1.3460612 foo two -1.143704 1.6270814 foo two 1.193555 -0.4416526 foo one -0.408530 0.2685207 foo three -0.862495 0.024580应用累计与求和最常用的方法有 .sum() 和 .size()，.sum() 是对其余所有数值型的列全部求和，.size() 是只计算聚合的列的个数。若对多列进行聚合，默认会把将要聚合的列当做一个 MultiIndex，这是因为在 groupby 的时候包含一个默认属性 as_index=True 这样的好处有两点：加快运算速度方便进一步的操作举例来说：In [13]: df = pd.DataFrame(data = &#123;1:['a','a','b','c','a'],2:['b','b','a','d','c ...: '],3:[1,1,1,1,1]&#125;)In [14]: dfOut[14]: 1 2 30 a b 11 a b 12 b a 13 c d 14 a c 1In [19]: df.groupby([1,2]).sum()Out[19]: 31 2a b 2 c 1b a 1c d 1In [20]: df.groupby([1,2],as_index=False).sum()Out[20]: 1 2 30 a b 21 a c 12 b a 13 c d 1而利用 reset_index() 可以达到同样的效果：In [24]: df.groupby([1,2]).sum().reset_index()Out[24]: 1 2 30 a b 21 a c 12 b a 13 c d 1但是对于 .size() 来说，设置 reset_index()=False 并没有效果，还是需要用 reset_index()，实际上，DataFrameGroupBy 对象 内置的.size() 方法返回的是一个 Series 对象而不是 DataFrameIn [18]: df.groupby([1,2],as_index=False).size() # 依旧是 MultiIndexOut[18]:1 2a b 2 c 1b a 1c d 1dtype: int64In [25]: df.groupby([1,2]).size().reset_index()Out[25]: 1 2 00 a b 21 a c 12 b a 13 c d 1若要将统计的 size 成为新的一列，可以使用 .to_frame 方法生成新的一列： df = df.groupby([&#39;A&#39;,&#39;B&#39;]).size().to_frame(&#39;size&#39;)转换与应用apply() 方法让你可以在每个组上应用任意方法。这个函数输入一个 DataFrame，返回一个 Pandas 对象（ DataFrame 或 Series ）或一个标量（ scalar，单个数值），新版的 Pandas 可以 放入 apply 的参数，例如：df.apply(f,alpha=0.1)-[]这里开始拾遗axis 取 0 还是 1？axis=1 表示按照行（index）的方向来，也就是说函数的输入为每一行，同理，axis=0 表示按照列（column）的方向来，也就是说函数的输入为每一列。df.apply() 输入 0 表示 apply 到每一列上去，输入 1 表示 apply 到每一行上去。sum(axis=1) 表示按行求和，sum(axis=0) 表示按列求和drop(axis = 1) 删除列正则表达式从开头匹配：表达式最前面加上^匹配年份：(?!0000)[0-9]{4}字符串Series.str 可以广播某一字符串方法，适用于整个列或者行，而避免用 apply 函数.在某一列寻找包含某一字符串 &#39;str&#39; 的数据：df[df[&#39;content&#39;].str.contains(&#39;str&#39;, na=False)]，由于 contains 返回布尔型数据，前面要再加上一个 df, na=False 表示若遇到缺失数据返回 False.strip() 方法能用于删除开始或结尾的字符。 lstrip() 和 rstrip() 分别从左和从右执行删除操作。 默认情况下，这些方法会去除空白字符，但是你也可以指定其他字符.时间序列操作生成固定日期的时间序列：pd.date_range生成固定时间段的时间序列：pd.date_rangeKaggle 数据竞赛常用命令pandas.Series.factorize()：返回一个 tuple，对于数据的某一列为种类属性，将里面的类别映射成数字:1234567In [0]: labels, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])In [1]: labelsOut[1]: array([0, 0, 1, 2, 0])In [2]: uniquesOut[2]: array(['b', 'a', 'c'], dtype=object)这个命令一般用于数据探索阶段，例如当给定的数据不知道是否为 categorical 类型的数据时，可以先进行一个映射：123# Label encoderfor c in train.columns[train.dtypes == 'object']: X[c] = X[c].factorize()[0]将训练集中的特征进行区分：cat_cols = list(train.select_dtypes(include=['object']).columns)num_cols = list(train.select_dtypes(exclude=['object']).columns)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook 使用方法记录]]></title>
    <url>%2Fposts%2F54421%2F</url>
    <content type="text"><![CDATA[“A web-based application suitable for capturing the whole computation process.”Jupyter Notebook 简介Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，而 Jupyter这个名字是它要服务的三种语言的缩写：Julia，Python 和 R，这个名字与 “木星（Jupiter）” 谐音。而 Jupyter Notebook 最早是为了支持 Python 设计的，目前为止支持运行 40 多种编程语言。其本质是一个 Web 应用程序，便于创建和共享程序文档，支持实时代码的运行。而且非常适合用来做流式的数据分析和尝试性的程序编写。而在普通的 Python shell 或者在 IDE（集成开发环境）如 Pycharm 中写代码会显得更加繁琐。作为一个 Web 应用程序，还可以实现远程服务器访问，也就是说可以配置jupyter 服务器来实现远程浏览器登录，还可以支持多人团队的 Jupyter Hub 实现服务器资源分配，关于 jupyter 远程登录可以参考这篇文章，前提是服务器必须有公网 IP，若没有，可以参考各种内网穿透的方法。作为 Web 服务器，若 8888 端口没有占用，Jupyter Notebook 会默认将 8888 设定为通信端口，若占用，则会将端口号 +1，有的时候在 shell 内无法停止端口占用，可以手动停止端口占用以免浪费端口。这里以 Windows 为例：例如要停止 8889 端口，则可以在 cmd 下输入 netstat -aon|findstr &quot;8889&quot; 找到对应的 PID，然后在任务管理器下结束对应 PID 的进程，即可完成端口释放。另外，Jupyter Notebook 非常适合作为教学工具，因为 markdown 的支持极大地丰富了文字表现力，现在 也有人将技术教学视频“翻译”成可执行的 .ipynb 文档，在类 IDE 环境中边看视频边实操运行代码。如 Mo 平台笔记本扩展功能笔记本扩展（nbextensions）是一种 JavaScript模块，可以加载到笔记本前端页面上，可以大大提升效率。安装时需要用到conda：conda install -c conda-forge jupyter_nbextensions_configurator总之，安装合适的扩展功能可以极大地方便代码书写。Jupyter NoteBook 的快捷键Jupyter在顶部的菜单里保留了许多快捷键：Help &gt; keyboard Shortcuts. 每次更新Jupyter时，都值得再次进行查看，因为新的快捷键总是不断被添加进来。查看快捷键的方式是使用命令面板：Cmd + Shift + P （或者 Linux 和Windows上 Ctrl+ Shift + P）Jupyter Notebook 有两种键盘输入模式。即命令模式和编辑模式，这与 Vim 有些类似。在编辑模式下，可以往单元中键入代码或文本，此时单元格被绿色的框线包围，且命令模式下的快捷键不生效。在命令模式下，可以用快捷键命令运行单元格，移动单元格，切换单元格编辑状态等等，此时的单元格被灰色的框线包围，且编辑模式下的快捷键不生效。从命令模式进入编辑模式需按 Enter 键，从编辑模式切换到命令模式需按 Esc 键。命令模式快捷键（按 `Esc` 键开启）:在当前 cell 下方创建新的 cell：A在当前 cell 上方创建新的 cell：B删除选中的单元：连续按两个 D进入 Markdown 状态：M恢复最后一个被删除的 cell：Z编辑模式快捷键（ 按 Enter 键启动）:给出当前函数提示：Shift + Tab注释（反注释）当前的代码行：Ctrl + /删除当前光标的全部文字：Command + Delete常用的 Jupyter Magic常用的 Jupyter Magic 魔法方法总结%load test.py：将当前路径下 test.py 载入到 jupyter 的当前 cell 中%reset -f：清除所有已经定义过的变量%%writefile filename.py：写在 cell 的开头，运行时保存为 filename.py 文件，路径默认为当前工作路径cell 内画图：%matplotlib inline使得 cell 内输出图形为矢量图：%config InlineBackend.figure_format = &#39;svg&#39;%store data: 保存 data 变量到磁盘，可以用在不同的 NoteBook 之间传递变量，适合短期保存（可 pickle 化的）数据%store: 查看已保存的变量%store -r: 从磁盘里已保存好的文件更新现在所有的变量，会覆盖当前同名的变量%store -r data: 更新覆盖某一个变量%store -d data: 移除已保存的 data%store -z: 移除全部保存的的变量变量保存在 ~/.ipython/profile_default/db/autorestore/&lt;variable_name&gt; ，~ 指 home 文件夹，并不适合用于资料的长期保存，可能会受到版本迭代的影响.利用魔法命令进行 Python 代码分析%timeit: 对某一行语句多次重复计算代码运算时间，多于较慢的命令会自动调整重复次数，其中在底层做了一些聪明的事情来阻止系统的垃圾回收，所以 %timeit 通常比 %time 得到更短的时间%time: 对某一行语句多次重复计算代码运算时间，主要关心 Wall time%%timeit、%%time: 对整个代码块计时%prun: 对每行单独计算时间分析，返回时间统计表%who_ls: 查看当前命名空间内的全部变量%reset_selective -f regex: 清除变量名称符合正则表达式的变量，-f 表示不询问强制执行.%reset -f : 清除所有变量升级到 Jupyter Lab！JupyterLab 是 Jupyter Notebook 的下一代产品，集成了更多功能。越来越像一个 类似于 MATLAB 的 IDE，目前开发到 0.35 版本，已经可以使用而且非常强大，据官网说法，当 1.0.0 版本出来之时，就会取代 jupyter notebook.将服务器升级为 Jupyter Lab 非常简单，只需要在服务器上安装好后在shell 内输入 jupyter lab, 原来 Notebook 的设置完全不改变，远程输入服务器地址即可访问 Jupyter Lab.JupyterLab 最大的好处就在于，对于多个 Notebook 可以使用同一个 Kernel ，也就是说，数据科学家需要一边尝试一边写核心代码，这样可以分开为两个 Notebook 而不显得凌乱.Jupyter Lab 无法兼容原来 NoteBook 的插件，需要额外安装 Jupyter Lab 插件，更多插件请参考 Jupyter RenderersNotes:查看所安装的插件可以运行 jupyter labextension list卸载某插件可以用 jupyter labextension uninstall，例如：jupyter labextension uninstall @jupyterlab/plotly-extension1.[译] 27 个 Jupyter Notebook的小提示与技巧 | Focus on ML &amp; DM ↩2.Jupyter Notebook的27个秘诀，技巧和快捷键 | 作业部落 Cmd Markdown 编辑阅读器 ↩3.JupyterLab插件 | RexKing6’s Note ↩]]></content>
      <tags>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让博客成为你的“全平台”笔记本]]></title>
    <url>%2Fposts%2F40915%2F</url>
    <content type="text"><![CDATA[“再好的表达能力也需要精致的排版技巧”1. 为什么使用 Hexo 排版hexo 是使用 Markdown 语言作为主要书写语言，其目的是通过简单、易读易写的文本格式生成结构化的 HTML 文档，所以 Markdown 是兼容 HTML 的。虽然一些基本的语法在各种 Markdown 书写软件是支持的，但是不同的软件支持的广度又不一样，例如 Github Flavored Markdown (GFM) 语法不支持 Markdown 注脚，这会造成易用性的困难，Hexo 可以很好地解决这个排版问题，例如可以通过安装相应的插件实现。利用 hexo server 进入调试模式，右边网页实时渲染 control+R 刷新，左边书写 Markdown control+S 保存，必要时可以利用移动设备局域网下访问站点或利用 Chrome 调试成移动端观察渲染效果。也有人将这种写作方式制作成一个后端管理工具，例如 hexo admin，不过相比直接网页渲染，所支持的渲染效果并不好.利用 VSCode 可以使用 Snippets 方便输入以下快捷命令TODO2. Hexo 文章模板文件如果你是在站点文件夹根目录用 hexo new post &lt;title&gt; 新建的文章，那么其实它就是将文章的模版文件 post.md「复制」了一份到 ~/blog/source/_posts/ 下，所以这也意味着：你可以通过命令行的方式创建模板样式的 .md 文件。你可以直接通过在 ~/blog/source/_posts/ 下新建 .md 结尾的文件来写新的文章。为了美观起见，英文和中文之间必须添加空格首先这里附上我的模板文件文件位置：~/hexo/blog/scaffolds/post.md1234567891011121314151617181920212223---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;permalink:categories:tags: []description:mathjax: ---&lt;p class="description"&gt;&lt;/p&gt;&lt;img src="" alt="" style="width:85%" /&gt;&lt;!-- more --&gt;######&lt;hr /&gt;&#123;% note success no-icon %&#125; &lt;center&gt;本文更新于 &lt;/center&gt;&#123;% endnote %&#125;Tips: 这个地方写作时若直接插入上面 Markdown 代码，{{ title }}、{{ date }} 的部分会被转义，所以应该在代码中间插入 {% raw %}{% endraw %} 防止被转义。下面逐一解释上述模板文件以 ---分隔的区域，叫做 Front-matter ，是用于指定此文件所包含的变量，具体变量实现可以百度搜索了解Hexo 会自动识别 &lt;!-- more --&gt; 部分为摘要，也就是表现在首页部分，一般来说，为了美观起见，首页都是一句话加上一张图片排版，这句话写在 &lt;p&gt;&lt;/p&gt; 标签内，src 插入图片的 url 链接。正文中所有的标题都是从二级标题开始。最后插入 &lt;hr /&gt; 横线表示文章结束，从后面可以插入参考文献。最后插入本文的修改时间，技术总是会随着时间更新，提示读者本文的最后修改时间。当然，文章摘要还需要在 _custom.styl 下插入一些自定义 CSS 样式，代码如下：文件位置：~/hexo/blog/themes/next/source/css/_custom/custom.styl12345678910111213141516+ // 文章的描述description+ .posts-expand .post-meta .post-description &#123;+ font-style: italic;+ font-size: 14px;+ margin-top: 30px;+ margin-bottom: 0px;+ color: #666;+ &#125;+ // 自定义替代description的样式+ p.description&#123;+ text-align: center;+ font-size: 14px;+ font-style: italic;+ color: #666;+ margin-top: 30px;+ &#125;红色 - 和绿色 + 的样式哪来的？哈哈哈，原来这也是一种语言，叫 diff，所以你只需在 [language] 这写 diff，然后在相应代码前面加上 - 和 + 就行了。不过默认的 - 是绿色，+ 是红色，与 GitHub 上相反，别扭就自己改成 GitHub 的，在 custom.styl 加入以下代码文件位置：~/hexo/blog/themes/next/source/css/_custom/custom.styl1234567+ // 文章```代码块diff样式+ pre .addition &#123;+ background: #e6ffed;+ &#125;+ pre .deletion &#123;+ background: #ffeef0;+ &#125;3. 段内文字格式Hexo 中基本的 Markdown 语法手册可以参考这篇博文 Hexo Markdown 简明语法手册 | Mobilicorpus，如果有些用 Markdwon 的语法却达不到预期效果（甚至产生奇怪的 bugs），或者用 Markdwon 的语法无法实现，这时就可以考虑用 HTML 和 CSS。下面是比较常见的，难以用 Markdwon 的语法实现的 HTML 语言：样式代码效果分隔线&lt;hr /&gt;引用&lt;blockquote&gt;引用内容&lt;/blockquote&gt;引用内容更宽的引用&lt;p&gt;&lt;blockquote&gt;引用内容&lt;/blockquote&gt;&lt;/p&gt;引用内容居中&lt;center&gt;内容&lt;/center&gt;内容右对齐&lt;p style=&quot;text-align:right&quot;&gt;内容&lt;/p&gt;内容左对齐&lt;p style=&quot;text-align:left&quot;&gt;内容&lt;/p&gt;内容字体大小和颜色1&lt;font color=&quot;red&quot; size=&quot;1&quot;&gt;内容&lt;/font&gt;内容黄色荧光笔&lt;mark&gt;内容&lt;/mark&gt;内容3.1 Bootstrap Callout2本站用的是 Next 主题，其配备了一些自带的主题样式，需要在主题配置文件 中开启：文件位置：~/hexo/blog/themes/next/config123456789101112note: # Note tag style values: # - simple bs-callout old alert style. Default. # - modern bs-callout new (v2-v3) alert style. # - flat flat callout style with background, like on Mozilla or StackOverflow. # - disabled disable all CSS styles import of note tag. style: simple icons: false border_radius: 3 # Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6). # Offset also applied to label tag variables. This option can work with disabled note tag. light_bg_offset: 0可以获得类似 beamer 中的 block 样式：This is a notenote textnote text代码如下：&#123;% note danger %&#125;#### This is a notenote textnote text&#123;% endnote %&#125; 注意：可以在 danger 后面加上 no-icon 实现去掉旁边的 icon，这个地方我用了四级标题 ####，避免歧义，左侧导航栏没有显示，在主题配置文件 toc 选项可以修改，同样，在附上这段代码时,同样加上 {% raw %}{% endraw %}，防止被转义。还有一种 label 样式，所谓的 label 从效果上看神似荧光笔，例如：这是default这是primary这是success这是info这是warning这是danger这是有删除线的danger相应的代码：1234567&#123;% label default@default%&#125;&#123;% label primary@primary%&#125;&#123;% label success@success%&#125;&#123;% label info@info%&#125;&#123;% label warning@warning%&#125;&#123;% label danger@danger%&#125;~~&#123;% label danger@danger%&#125;~~ 3.2 代码格式在主题配置文件可以开启代码自动高亮功能，代码按照以下格式给出123&#123;% codeblock [title] [lang:language] [url] [link text] [line_number:(true|false)] [highlight:(true|false)] [first_line:number] [mark:#,#-#] %&#125;YOUR CODE HERE&#123;% endcodeblock %&#125; title 是代码名称，一般为此段代码所在的文件路径；language 是代码语言的名称，用来设置代码块颜色高亮，纯文本用 plain；url 是文件所在的超链接地址；link text 如它的字面意思，超链接的名称；注意：这四个参数都不是必须的，若不指定代码语言开启自动高亮后则会自动为代码高亮更多内置标签插件样式参考 标签插件4. 段间排版对于中文排版，常用的排版方式分为两种，一种是首段空两格，段间距等于行间距，这种方式叫做段首缩排（下图左一）。还有一种是段首不进行缩进，段间距大于行间距，叫做段间距式（下图左二）。书籍印刷的表现要求是专注文字内容，因此阅读体验应当置于美观之上。由于段间距比段首缩排的分离程度更大，一字连一字阅读时易产生隔阂感、停顿感，故段首缩排与之相比更为优雅，有着更好的阅读体验。对于网页，与纸质书的翻页不同，它是竖直方向的连续滚动。当网页快速滚动时，就使段落上下相连的段首缩排显得繁密易懵，而段落上下间隔的段间距则显得结构简明。段首缩排在快速滚动的互联网时代显得混乱与不整齐，段间距也就比段首缩排更适合于网页，因为它更利于快速传达信息。不要同时用段首缩排和段间距！(上图右一)，技术类文章用段间距排版（左二），文学类的文章用段首缩排（左一）！5. Mathjax\| 变为 \\|1.HTML 标签 | w3school ↩2.Note (Bootstrap Callout) | NexT ↩3.打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化 | reuixiy ↩]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>博客搭建</tag>
      </tags>
  </entry>
</search>
